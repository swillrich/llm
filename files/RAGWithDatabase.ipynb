{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9eb3e93-9570-48a4-8bd0-825f7ad4d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def get_cursor():\n",
    "    # Connect to DB\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"vector_db\",\n",
    "        user=\"user\",\n",
    "        password=\"123456\",\n",
    "        host=\"pgvector\",\n",
    "        port=5432\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # 1. Enable pgvector extension\n",
    "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "    return cur, conn\n",
    "\n",
    "def select(sql_statement):\n",
    "    cur, conn = get_cursor()\n",
    "    cur.execute(sql_statement)\n",
    "    res = cur.fetchall()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86c499a8-d49b-4c06-95dc-0177820395a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#1 (distance=1.0000)\n",
      "\"></p><p></p><p></p>...\n",
      "\n",
      "#2 (distance=1.0000)\n",
      "p>...\n",
      "\n",
      "#3 (distance=1.0000)\n",
      "p><p></p>...\n",
      "\n",
      "#4 (distance=1.0093)\n",
      "<p>Bitte sorgt dafür, dass der Ständer täglich mit den neuesten Ausgaben befüllt ist.</p><p><img src=\"/geteilte-inhalte/9cea8aa0-558c-4da0-8cf2-2e51d9d2a6e4?showFileContent=1&amp;p=medium\"></p><p></p><p></p>...\n",
      "\n",
      "#5 (distance=1.3180)\n",
      "<p>Zur Präsentation eines vollen und abwechslungsreichen <strong>Sortimentes</strong>, wie es in jeder unserer Bäckereifiliale zu finden ist, gehört auch, dass die sonst so trübe und leblos erscheinende Thekenablage, welche ständig im Blickfeld des Kunden ist entsprechend genutzt wird.</p><p><img sr...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import psycopg2\n",
    "\n",
    "import joblib\n",
    "global vectorizer\n",
    "vectorizer = joblib.load('vectorizer.joblib')\n",
    "\n",
    "def get_similar_entries(query):\n",
    "    q_vec = vectorizer.transform([query]).toarray()[0]\n",
    "    \n",
    "    q_vec_str = str(list(q_vec))\n",
    "    \n",
    "    # Run similarity search using the <-> operator (Euclidean distance) \n",
    "    cur, conn = get_cursor()\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT id, content, embedding <-> %s::vector AS distance\n",
    "        FROM documents\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT 5\n",
    "    \"\"\", (q_vec_str,))\n",
    "\n",
    "    return cur.fetchall()\n",
    "\n",
    "query = \"Wie oft sollen wir dafür sorgen, dass der Ständer mit neusten Aufgaben befüllt ist?\"\n",
    "\n",
    "results = get_similar_entries(query)\n",
    "\n",
    "for rank, (doc_id, content, distance) in enumerate(results, 1):\n",
    "    print(f\"\\n#{rank} (distance={distance:.4f})\\n{content[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9197b4d5-de6d-4b23-9b58-ea484298cfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Um einen effektiven und kontinuierlichen Arbeitsfluss zu gewährleisten, sollte der Ständer regelmäßig mit neuen Aufgaben befüllt werden. Je nach Größe und Art der Projekte kann dies täglich, wöchentlich oder monatlich passieren. Es gibt kein allgemeingültiges Regelwerk dafür, wie oft ein Ständer mit neuen Aufgaben befüllt werden soll. Hierbei ist wichtig, sich die Arbeitsbelastung und den Ablauf der Projekte anzupassen.\n",
      "\n",
      "Es empfele ich folgende Prinzipien:\n",
      "\n",
      "1. Priorisierung: Verwende prioritätsbasierte Arbeitsspeicherung, damit du die wichtigsten Aufgaben im Ständer bekommst und nicht vergessen kannst.\n",
      "2. Flexibilität: Sei flexibel und passe deine Prioritäten an die Änderungen in der Arbeit oder den Umständen an.\n",
      "3. Effizienz: Nutze deinen Ständer effizient, um sich auf wichtige Tätigkeiten zu konzentrieren und Zeit zu sparen.\n",
      "4. Veränderbarkeit: Verwende einen flexiblen Systemansatz, damit du leicht darauf reagieren kannst, wenn neue Aufgaben auftauchen oder deine Prioritäten sich ändern.\n",
      "5. Kommunikation: Kommuniziere mit deinen Kollegen und teile ihnen den Inhalt des Ständer mit, um gemeinsam auf die Arbeit zugreifen zu können.\n",
      "\n",
      "⏱️ Time taken: 105.59 seconds\n"
     ]
    }
   ],
   "source": [
    "#import requests\n",
    "import time\n",
    "#!pip install scikit-learn llama-cpp-python\n",
    "\n",
    "def query_llm(model, prompt):\n",
    "    url = \"http://ollama:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, json=payload)\n",
    "    end_time = time.time()\n",
    "\n",
    "    result = response.json()[\"response\"]\n",
    "    duration = end_time - start_time\n",
    "\n",
    "    print(f\"Response: {result}\")\n",
    "    print(f\"\\n⏱️ Time taken: {duration:.2f} seconds\")\n",
    "\n",
    "query_llm(\"mistral\", \"Wie oft sollen wir dafür sorgen, dass der Ständer mit neusten Aufgaben befüllt ist?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "208692b8-2be6-40e7-90de-c08c3bf05f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following using the provided context.\n",
      "\n",
      "Context:\n",
      "#1:\n",
      "\"></p><p></p><p></p>\n",
      "\n",
      "#2:\n",
      "p>\n",
      "\n",
      "#3:\n",
      "p><p></p>\n",
      "\n",
      "#4:\n",
      "<p>Bitte sorgt dafür, dass der Ständer täglich mit den neuesten Ausgaben befüllt ist.</p><p><img src=\"/geteilte-inhalte/9cea8aa0-558c-4da0-8cf2-2e51d9d2a6e4?showFileContent=1&amp;p=medium\"></p><p></p><p></p>\n",
      "\n",
      "#5:\n",
      "<p>Zur Präsentation eines vollen und abwechslungsreichen <strong>Sortimentes</strong>, wie es in jeder unserer Bäckereifiliale zu finden ist, gehört auch, dass die sonst so trübe und leblos erscheinende Thekenablage, welche ständig im Blickfeld des Kunden ist entsprechend genutzt wird.</p><p><img sr\n",
      "\n",
      "Question:\n",
      "Wie oft sollen wir dafür sorgen, dass der Ständer mit neusten Aufgaben befüllt ist?\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n",
      "Response:  Täglich sollte der Ständer mit den neuesten Ausgaben befüllt werden.\n",
      "\n",
      "⏱️ Time taken: 29.85 seconds\n"
     ]
    }
   ],
   "source": [
    "def ask_with_context(query, model=\"mistral\"):\n",
    "    results = get_similar_entries(query)\n",
    "\n",
    "    # Build context from top_k entries\n",
    "    context = \"\\n\\n\".join([f\"#{i+1}:\\n{content}\" for i, (doc_id, content, dist) in enumerate(results)])\n",
    "\n",
    "    # Build the final prompt\n",
    "    prompt = f\"\"\"Answer the following using the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    print(prompt)\n",
    "    print()\n",
    "    print()\n",
    "    query_llm(model=model, prompt=prompt)\n",
    "\n",
    "ask_with_context(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
